{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fec3159",
   "metadata": {},
   "source": [
    "# Web scraping: THE WORLD FACTBOOK\n",
    "\n",
    "## Introducción\n",
    "\n",
    "\"The World Factbook\" de la CIA es una publicación en línea que proporciona información detallada sobre diversos países y territorios de todo el mundo. Es una referencia ampliamente utilizada y una de las fuentes de información más completas y confiables sobre datos demográficos, geográficos, económicos, políticos y militares de diferentes naciones.  \n",
    "<br>\n",
    "La página web de \"The World Factbook\" de la CIA ofrece un amplio conjunto de datos sobre más de 250 países y territorios (262 para el 14 de junio del 2023). Proporciona información sobre aspectos como la geografía, la población, el gobierno, la economía, las comunicaciones, las fuerzas militares y mucho más. Estos datos incluyen descripciones, estadísticas, gráficos y mapas que permiten obtener una visión general de cada país o territorio.\n",
    "\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "### Generales\n",
    "- Recopilar datos desde una página web por medio de métodos de programación tradicional.\n",
    "\n",
    "### Especifícos\n",
    "- Analizar la estructura del contenidos de la página web `The World Factbook`. \n",
    "- Extraer datos mediante el uso de técnicas de web scrapping utilizando el lenguaje de programación Python.\n",
    "- Modelar y diseñar la base de datos que almacenará los datos sustraidos de la página web `The World factbook`.\n",
    "\n",
    "\n",
    "## Descripción del problema\n",
    "El objetivo de este proyecto es realizar un proceso de Web Scraping en la página web \"[The World factbook](https://www.cia.gov/the-world-factbook/)\" para extraer datos relevantes sobre diversos países. Utilizando el lenguaje de programación Python y la biblioteca BeautifulSoup, se llevará a cabo la extracción y procesamiento de la información disponible en la página web.\n",
    "\n",
    "Una vez obtenidos los datos, se almacenarán en un archivo plano en formato JSON, lo cual permitirá su posterior manipulación y análisis. El siguiente paso consistirá en realizar un proceso de ETL (Extracción, Transformación y Carga) para transferir los datos extraídos desde el archivo JSON a una base de datos relacional.\n",
    "\n",
    "La base de datos relacional será utilizada como fuente de información para responder a las preguntas planteadas en el contexto del proyecto. Mediante consultas a la base de datos, se podrán obtener estadísticas, características y visualizaciones relevantes sobre los países y sus atributos.\n",
    "\n",
    "Este proyecto tiene como finalidad demostrar las habilidades en Web Scraping, procesamiento de datos y manejo de bases de datos relacionales. Además, proporcionará una oportunidad para aplicar técnicas de visualización de datos y responder preguntas analíticas a partir de la información recopilada.\n",
    "\n",
    "A través de este proyecto, se espera que los participantes adquieran experiencia práctica en la obtención de datos, manipulación de información, modelado de datos y generación de conocimiento a partir de los mismos.\n",
    "\n",
    "\n",
    "## 1. - Actividades \n",
    "\n",
    "1. **Exploración del sitio web [The World factbook](https://www.cia.gov/the-world-factbook/)**\n",
    "    - Análisis inicial: Revisar, leer y explorar el sitio web.\n",
    "    - Revisar, leer y explorar la estructura `HTML` del sitio web.\n",
    "    - Reconocer diferentes etiquetas de contenido.\n",
    "    -  **Análisis**: Es necesario estudiar la estructura del `HTML` para la pagina web `The world factbook`, esta estructura es una pauta _inicial-esencial_ para la identificación de las etiquetas  que contienen caracteristicas de interes. Por ejemplo:\n",
    "       - titulos contenidos en etiquetas `<h2></h2>`\n",
    "       - subtitulos contenidos en etiquetas `<h3></h3>`\n",
    "       - parrafos contenidos en etiquetas `<p></p>`\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "2. **Extracción de los datos**\n",
    "    - Haciendo uso de la herramienta `Jupyter Notebook` programar los módulos necesarios que responden a la necesidad de extraer el contenido de la página web.\n",
    "    - Extraer de la página web los datos identificados a partir del lenguaje de programación Python utilizando la `librería BeautifulSoup`.\n",
    "    - Extraer la información de cada uno de los países (historia, población, gobierno, economía, geografía, medio ambiente, comunicaciones, transporte...).\n",
    "    - Mediante el uso de python, hacer un pre procesamiento de los datos (extracción y limpieza, es decir obtener el texto plano sin etiquetas HTML, sin espacios en blanco).\n",
    "    - Guardar los datos para cada país en un archivo (único) plano en formato JSON.\n",
    "        - Respetando la estructura `clave:valor` del archivo con formato JSON\n",
    "    \n",
    "<br>\n",
    "    \n",
    "3. **SQL**\n",
    "    - Crear un modelo `entidad relación` que responda a las preguntas de negocio (*leer detenidamente el inciso 5*).\n",
    "        - Crear la entidad.\n",
    "        - Identificar atributos.\n",
    "    - Esquematizar la base de datos, pasar del modelo `Entidad relación` al `lenguaje de definición de datos` (DDL).\n",
    "    - Crear una base de datos en MySQL con el nombre `IIDDBDWorldFactbook`.\n",
    "    - Crear una tabla en la base de datos `IIDDBDWorldFactbook` con el nombre `WorldFactbookDataAnalysis` con sus correspondientes campos, tipos de datos, etc.\n",
    "    \n",
    "**Nota**:\n",
    "Será importante identificar los campos que respondan a las preguntas exploratorias del inciso 5. Estos campos se refieren a los datos estructurados contenidos en el archivo plano en formato JSON. <br>\n",
    "Por ejemplo, `¿Cuáles son los países más y menos poblados?` quiere decir que existirá un atributo en la tabla `WorldFactbookDataAnalysis` llamado `population` que contiene el número total de población correspondiente a cada país. \n",
    "\n",
    "<br>\n",
    "    \n",
    "4. **ETL**\n",
    "    - Mediante el uso de la herramienta de integración de datos, Talend, realizar un ETL (Extracción, Transformación y Carga) al archivo plano en formato JSON que contiene la información de los países.\n",
    "    - Mapear los datos (keys) del archivo plano en formato JSON con los campos de la tabla `WorldFactbookDataAnalysis`. \n",
    "    - Entregar un único archivo que es un JOB con los componentes que realicen lo pedido. \n",
    "    - Debe entregar una captura de pantalla con el JOB que contiene los componentes (una única imagen) con el nombre `etl.png`\n",
    "    \n",
    "    \n",
    "<br>\n",
    "\n",
    "5. **Responder a las preguntas para explorar los datos**  \n",
    " \n",
    "    5.1. *Población:*  \n",
    "    - ¿Cuáles son los países más y menos poblados?\n",
    "    - ¿Cuál es la densidad de población promedio por región o continente?\n",
    "    - ¿Cuáles son los países con la tasa de crecimiento demográfico más alta o más baja?  \n",
    "    \n",
    "    5.2. *Economía:*\n",
    "    - ¿Cuáles son las principales economías del mundo en términos de PIB?\n",
    "    - ¿Cuáles son los países con el ingreso per cápita más alto o más bajo?\n",
    "    - ¿Cuáles son los sectores económicos más importantes en diferentes países?  \n",
    "    \n",
    "    5.3. *Geografía:*\n",
    "    - ¿Cuáles son los países más grandes y más pequeños en términos de área?\n",
    "    - ¿Cuál es la longitud de las costas de diferentes países?\n",
    "    - ¿Cuáles son los países con la altitud más alta o más baja?  \n",
    "    \n",
    "    5.4. *Educación:*\n",
    "    - ¿Cuáles son los países con los niveles más altos de alfabetización?\n",
    "    - ¿Cuál es la tasa de matriculación escolar en diferentes países?\n",
    "    - ¿Cuáles son los países con la mayor inversión en educación?  \n",
    "    \n",
    "    5.5. *Salud:*\n",
    "    - ¿Cuál es la esperanza de vida promedio en diferentes países?\n",
    "    - ¿Cuáles son los países con la tasa de mortalidad infantil más alta o más baja?\n",
    "    - ¿Cuáles son los principales problemas de salud en diferentes regiones?\n",
    "\n",
    "<br>\n",
    "\n",
    "6. **Resultados y conclusiones**\n",
    "   - Se espera un único archivo `ipynb` llamado `webScrapingCIA.ipynb`.\n",
    "   - Una carpeta llamada `core` que contendrá los paquetes de programaciñn desarrollados por el estudiante y que responden a la parte de extracción de los datos. \n",
    "   - Una carpeta llamada `data` que contendrá el archivo plano en formato JSON con la información extraída del sitio web [The World factbook](https://www.cia.gov/the-world-factbook/).\n",
    "   - Una carpeta llamada `SQL` que contendrá la definición de la base datos, la solución a las preguntas exploratorias y el modelo relacional de la base de datos que será exportado aplicando ingeniería inversa a la base de datos mediante la herramienta de visualización de MySQL.\n",
    "   - Una carpeta llamada `ER` dentro de la carpeta `SQL` que contiene el modelo entidad-relación con nombre `er.png` este corresponde a la tabla de la base de datos `IIDDBDWorldFactbook`.\n",
    "   - Una carpeta llamada `ETL` que contendrá el JOB y una captura de pantalla con el JOB que contiene los componentes (una única imagen) con el nombre `etl.png`\n",
    "   - El siguiente es un ejemplo de la estructura de carpetas de lo esperado:  \n",
    "     - ETL\n",
    "       - `world-factbook.item`\n",
    "       - `etl.png`\n",
    "     - SQL\n",
    "       - `dml-web-scraping.sql`\n",
    "       - `ddl-web-scraping.sql`\n",
    "       - ER\n",
    "           - `er.png`\n",
    "     - data\n",
    "       - `world-factbook.json`\n",
    "     - core\n",
    "       - `ExtractHTML.py`\n",
    "       - `ProcessHTML.py`\n",
    "       - `Tools.py`\n",
    "       - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac676d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from core.Tools import Tools\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Tools:\n",
    "    \"\"\"\n",
    "        @author Kenneth Cruz\n",
    "        @date 2023-06-20\n",
    "        @version 0.0.1\n",
    "        @source 2023-06-12\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def readFile(self, path:str, name:str) -> list:\n",
    "        \"\"\"\n",
    "            Lectura de un documento a partir de su ruta.\n",
    "        \"\"\"\n",
    "        f = open(f\"{path}/{name}\", mode=\"r\")\n",
    "        data = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def saveFile(self, path:str, name:str, data:dict)-> None:\n",
    "        \"\"\"\n",
    "            Guarda un documento dado la ruta y el nombre junto con la extemsion del archivo.\n",
    "        \"\"\"\n",
    "        f = open(f\"{path}/{name}\", mode=\"w\")\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "        print(\"Guardado con exito\")\n",
    "        \n",
    "        \n",
    "    def saveFileJson(self, path:str, name:str, data:dict)-> None:\n",
    "        \"\"\"\n",
    "            Guarda un documento tipo json dado la ruta y el nombre junto con la extemsion del archivo.\n",
    "        \"\"\"\n",
    "        with open(f\"{path}/{name}\", \"w\") as file:\n",
    "            json.dump(data, file)\n",
    "        print(\"Guardado con exito\")\n",
    "        \n",
    "    def readFileJson(self, path:str, name:str) -> list:\n",
    "        \"\"\"\n",
    "            Lectura de un documento a partir de su ruta.\n",
    "        \"\"\"\n",
    "        with open(f\"{path}/{name}\", \"r\") as archivo:\n",
    "            data = json.load(archivo)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def conver_to_lower(self, word: str) -> str:\n",
    "        return word.strip().lower()\n",
    "        \n",
    "\n",
    "class ProcessInfo:\n",
    "    \n",
    "    def __init__(self, url: str):\n",
    "        self.url = url\n",
    "        \n",
    "    def get_info_by_contrie(self, countrie: str) -> str:\n",
    "        \"\"\"\n",
    "            get_content_HTML\n",
    "        \"\"\"\n",
    "        res = requests.get(f\"{self.url}/{countrie}\")\n",
    "        return res.text if res.status_code == 200 else f\"Error al obtener el contenido HTML. Status: {res.status_code}\"\n",
    "    \n",
    "    def get_info_by_class(self, html: str, name_class: str, element: str) -> str:\n",
    "        \"\"\"\n",
    "            get_important_info\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(\"\".join(html), \"html.parser\") \n",
    "        return soup.find(element, class_ = name_class)# div article-content\n",
    "\n",
    "    \n",
    "class AnalizerHTML:\n",
    "    \n",
    "    def __init__(self, content: dict):\n",
    "        self.content = content\n",
    "        \n",
    "    def get_html_by_contrie(self, contrie: str) -> str:\n",
    "        return self.content[contrie]\n",
    "    \n",
    "    def get_fragment_HTML(self, html: str, element: str) -> list:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return soup.find_all(element)\n",
    "    \n",
    "    def get_fragment_id_HTML(self, html: str, element: str) -> list:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        return soup.find_all(element, id=True)\n",
    "    \n",
    "    def get_div_content_by_id(self, html: str, div_id: str) -> str:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        div_element = soup.find(\"div\", id=div_id)\n",
    "        if div_element:\n",
    "            return div_element\n",
    "        else:\n",
    "            return \"No se encontró un div con el ID especificado.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98af0ea5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    baseURL = \"https://www.cia.gov/the-world-factbook/countries\"\n",
    "    \n",
    "    tools = Tools()\n",
    "    RAWCountries = tools.readFile(path=\".\", name=\"countries.txt\")\n",
    "\n",
    "    \"\"\"\n",
    "    p = ProcessInfo(baseURL)\n",
    "    \n",
    "    data = {\n",
    "        tools.conver_to_lower(countrie): \n",
    "            str(\n",
    "                p.get_info_by_class(\n",
    "                    p.get_info_by_contrie(p.conver_to_lower(countrie)),\n",
    "                    \"article-content\",\n",
    "                    \"div\"\n",
    "                )\n",
    "            ) for countrie in RAWCountries\n",
    "    }\n",
    "    \n",
    "    tools.saveFileJson(\"data\", \"datos_paises_html.json\", data)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83fc732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado con exito\n"
     ]
    }
   ],
   "source": [
    "def parse_p_tags(p):\n",
    "    result = {}\n",
    "    p = str(p)\n",
    "\n",
    "    soup = BeautifulSoup(p, 'html.parser')\n",
    "    strong_tags = soup.find_all('strong')\n",
    "\n",
    "    if len(strong_tags) == 0:\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        for strong_tag in strong_tags:\n",
    "            key = strong_tag.get_text()\n",
    "            sibling = strong_tag.find_next_sibling(string=True)\n",
    "            if sibling is not None:\n",
    "                value = sibling.strip()\n",
    "                result[key] = value\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "json_data = tools.readFileJson(\"data\", \"datos_paises_html.json\")\n",
    "\n",
    "soup = BeautifulSoup(json_data['angola'], 'html.parser')\n",
    "\n",
    "data = {}\n",
    "\n",
    "divs = soup.find_all('div', id=True)\n",
    "\n",
    "data = {}\n",
    "\n",
    "divs = soup.find_all('div', id=True)\n",
    "\n",
    "# Recorrer cada div encontrado\n",
    "for div in divs:\n",
    "    h2_value = div.find('h2').text.strip() # Valor del <h2>valor</h2>\n",
    "    h3_data = {}\n",
    "\n",
    "    h3_tags = div.find_all('h3')\n",
    "    for h3_tag in h3_tags:\n",
    "        h3_value = h3_tag.text.strip() # Valor del <h3>valor</h3>\n",
    "        p_value = h3_tag.find_next('p').text.strip() # Valor del <p>valor</p>  \n",
    "        h3_data[h3_value] = parse_p_tags(h3_tag.find_next('p'))\n",
    "\n",
    "    data[h2_value] = h3_data # Agrega el json como key: h2: value: TODO lo que se proceso en los tags de h3\n",
    "        \n",
    "        \n",
    "tools.saveFileJson(\"data\", \"prueba.json\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4321c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado con exito\n"
     ]
    }
   ],
   "source": [
    "def parse_p_tags(p):\n",
    "    result = {}\n",
    "    p = str(p)\n",
    "\n",
    "    soup = BeautifulSoup(p, 'html.parser')\n",
    "    strong_tags = soup.find_all('strong')\n",
    "\n",
    "    if len(strong_tags) == 0:\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        for strong_tag in strong_tags:\n",
    "            key = strong_tag.get_text()\n",
    "            sibling = strong_tag.find_next_sibling(string=True)\n",
    "            if sibling is not None:\n",
    "                value = sibling.strip()\n",
    "                result[key] = value\n",
    "    \n",
    "    return result\n",
    "    \n",
    "\n",
    "json_data = tools.readFileJson(\"data\", \"datos_paises_html.json\")\n",
    "\n",
    "data_to_save = {}\n",
    "\n",
    "for countrie in RAWCountries:\n",
    "    data = {}\n",
    "    \n",
    "    soup = BeautifulSoup(json_data[tools.conver_to_lower(countrie)], 'html.parser')\n",
    "    divs = soup.find_all('div', id=True)\n",
    "\n",
    "    # Recorrer cada div encontrado\n",
    "    for div in divs:\n",
    "        h2_value = div.find('h2').text.strip() # Valor del <h2>valor</h2>\n",
    "        h3_data = {}\n",
    "        h3_tags = div.find_all('h3')\n",
    "        for h3_tag in h3_tags:\n",
    "            h3_value = h3_tag.text.strip() # Valor del <h3>valor</h3>\n",
    "            p_value = h3_tag.find_next('p').text.strip() # Valor del <p>valor</p>  \n",
    "            h3_data[h3_value] = parse_p_tags(h3_tag.find_next('p'))\n",
    "\n",
    "        data[h2_value] = h3_data # Agrega el json como key: h2: value: TODO lo que se proceso en los tags de h3\n",
    "        \n",
    "    data_to_save[tools.conver_to_lower(countrie)] = data\n",
    "        \n",
    "tools.saveFileJson(\"data\", \"world-factbook.json\", data_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "acf0df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado con exito\n"
     ]
    }
   ],
   "source": [
    "json_data = tools.readFileJson(\"data\", \"world-factbook.json\")\n",
    "elements = [\"Geography\", \"People and Society\", \"Economy\"]\n",
    "\n",
    "final_data = {}\n",
    "\n",
    "for countrie in RAWCountries:\n",
    "    country_lower = tools.conver_to_lower(countrie)\n",
    "    \n",
    "    if country_lower in json_data:\n",
    "        country_info = json_data[country_lower]\n",
    "        country_data = {}\n",
    "        \n",
    "        for element in elements:\n",
    "            if element in country_info:\n",
    "                country_data[element] = country_info[element]\n",
    "        \n",
    "        final_data[country_lower] = country_data\n",
    "    else:\n",
    "        print(f\"Data not found for {countrie}\")\n",
    "\n",
    "tools.saveFileJson(\"data\", \"world-factbook.json\", final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fa4ba94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'afghanistan': {'countrie': 'afghanistan', 'population': '39,232,003 (2023 est.)', 'Population growth rate': '2.26% (2023 est.)', 'area': '652,230 sq km', 'Map references': 'Asia', 'Coastline': '0 km (landlocked)', 'elevation highest point': 'Noshak 7,492 m', 'elevation lowest point': 'Amu Darya 258 m', 'economy': {'GDP': '$20.24 billion (2017 est.)', 'GDP-composition': {'agriculture': '23% (2016 est.)', 'industry': '21.1% (2016 est.)', 'services': '55.9% (2016 est.)'}}, 'education': {'school life expectancy': '10 years', 'Education expenditures': '2.9% of GDP (2020 est.)'}, 'health': {'life expectancy at birth': {'total population:': '54.05 years', 'male:': '52.47 years', 'female:': '55.71 years (2023 est.)'}, 'infant mortality rate': {'total:': '103.06 deaths/1,000 live births', 'male:': '111.47 deaths/1,000 live births', 'female:': '94.24 deaths/1,000 live births (2023 est.)'}, 'Health problems': {'Current health expenditure': '15.5% of GDP (2020)', 'Physicians density': '0.25 physicians/1,000 population (2020)', 'Hospital bed density': '0.4 beds/1,000 population (2017)'}}}}\n"
     ]
    }
   ],
   "source": [
    "json_data = tools.readFileJson(\"data\", \"world-factbook.json\")\n",
    "elements = [\"Geography\", \"People and Society\", \"Economy\"]\n",
    "\n",
    "countrie = json_data['afghanistan']\n",
    "\n",
    "\n",
    "c = {\n",
    "    'countrie': 'afghanistan',\n",
    "    'population': countrie['People and Society']['Population'],\n",
    "    'Population growth rate': countrie['People and Society']['Population growth rate'],\n",
    "    'area': countrie['Geography']['Area']['total:'],\n",
    "    'Map references': countrie['Geography']['Map references'],\n",
    "    'Coastline': countrie['Geography']['Coastline'],\n",
    "    'elevation highest point': countrie['Geography']['Elevation']['highest point:'],\n",
    "    'elevation lowest point': countrie['Geography']['Elevation']['lowest point:'],\n",
    "    'economy': {\n",
    "        'GDP': countrie['Economy']['GDP (official exchange rate)'],\n",
    "        'GDP-composition': {\n",
    "            'agriculture': countrie['Economy']['GDP - composition, by sector of origin']['agriculture:'],\n",
    "            'industry': countrie['Economy']['GDP - composition, by sector of origin']['industry:'],\n",
    "            'services': countrie['Economy']['GDP - composition, by sector of origin']['services:'],\n",
    "        }\n",
    "    },\n",
    "    'education': {\n",
    "        'school life expectancy': countrie['People and Society']['School life expectancy (primary to tertiary education)']['total:'],\n",
    "        'Education expenditures': countrie['People and Society']['Education expenditures'],\n",
    "    },\n",
    "    'health': {\n",
    "        'life expectancy at birth': countrie['People and Society']['Life expectancy at birth'],\n",
    "        'infant mortality rate': countrie['People and Society']['Infant mortality rate'],\\\n",
    "        'Health problems': {\n",
    "            'Current health expenditure': countrie['People and Society']['Current health expenditure'],\n",
    "            'Physicians density': countrie['People and Society']['Physicians density'],\n",
    "            'Hospital bed density': countrie['People and Society']['Hospital bed density'],\n",
    "#             'Sanitation facility access': {\n",
    "#                 'improved': {\n",
    "#                     'urban': countrie['People and Society']['Sanitation facility access']['improved:'],\n",
    "#                     'rural': countrie['People and Society']['Sanitation facility access']['improved:'],\n",
    "#                     'total': countrie['People and Society']['Sanitation facility access']['improved:']\n",
    "#                 },\n",
    "#                 'unimproved': {\n",
    "#                     'urban': countrie['People and Society']['Sanitation facility access']['improved:'],\n",
    "#                     'rural': countrie['People and Society']['Sanitation facility access']['improved:'],\n",
    "#                     'total': countrie['People and Society']['Sanitation facility access']['improved:']\n",
    "#                 }\n",
    "#             }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "result = {\n",
    "    'afghanistan': c\n",
    "}\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "be2d10bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado con exito\n"
     ]
    }
   ],
   "source": [
    "json_data = tools.readFileJson(\"data\", \"world-factbook.json\")\n",
    "elements = [\"Geography\", \"People and Society\", \"Economy\"]\n",
    "\n",
    "final_data = {}\n",
    "\n",
    "for pais in RAWCountries:\n",
    "    \n",
    "    country_lower = tools.conver_to_lower(pais)\n",
    "    countrie = json_data[country_lower]\n",
    "\n",
    "    c = {\n",
    "        'countrie': country_lower,\n",
    "        'population': countrie.get('People and Society', {}).get('Population', ''),\n",
    "        'Population growth rate': countrie.get('People and Society', {}).get('Population growth rate', ''),\n",
    "        'area': countrie.get('Geography', {}).get('Area', {}).get('total:', ''),\n",
    "        'Map references': countrie.get('Geography', {}).get('Map references', ''),\n",
    "        'Coastline': countrie.get('Geography', {}).get('Coastline', ''),\n",
    "        'elevation highest point': countrie.get('Geography', {}).get('Elevation', {}).get('highest point:', ''),\n",
    "        'elevation lowest point': countrie.get('Geography', {}).get('Elevation', {}).get('lowest point:', ''),\n",
    "        'economy': {\n",
    "            'GDP': countrie.get('Economy', {}).get('GDP (official exchange rate)', ''),\n",
    "            'GDP-composition': {\n",
    "                'agriculture': countrie.get('Economy', {}).get('GDP - composition, by sector of origin', {}).get('agriculture:', ''),\n",
    "                'industry': countrie.get('Economy', {}).get('GDP - composition, by sector of origin', {}).get('industry:', ''),\n",
    "                'services': countrie.get('Economy', {}).get('GDP - composition, by sector of origin', {}).get('services:', ''),\n",
    "            }\n",
    "        },\n",
    "        'education': {\n",
    "            'school life expectancy': countrie.get('People and Society', {}).get('School life expectancy (primary to tertiary education)', {}).get('total:', ''),\n",
    "            'Education expenditures': countrie.get('People and Society', {}).get('Education expenditures', ''),\n",
    "        },\n",
    "        'health': {\n",
    "            'life expectancy at birth': countrie.get('People and Society', {}).get('Life expectancy at birth', ''),\n",
    "            'infant mortality rate': countrie.get('People and Society', {}).get('Infant mortality rate', ''),\n",
    "            'Health problems': {\n",
    "                'Current health expenditure': countrie.get('People and Society', {}).get('Current health expenditure', ''),\n",
    "                'Physicians density': countrie.get('People and Society', {}).get('Physicians density', ''),\n",
    "                'Hospital bed density': countrie.get('People and Society', {}).get('Hospital bed density', ''),\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    final_data[country_lower] = c\n",
    "\n",
    "tools.saveFileJson(\"data\", \"world-factbook.json\", final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "492dbf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = tools.readFileJson(\"data\", \"world-factbook.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a72cd54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countrie': 'afghanistan',\n",
       " 'population': '39,232,003 (2023 est.)',\n",
       " 'Population growth rate': '2.26% (2023 est.)',\n",
       " 'area': '652,230 sq km',\n",
       " 'Map references': 'Asia',\n",
       " 'Coastline': '0 km (landlocked)',\n",
       " 'elevation highest point': 'Noshak 7,492 m',\n",
       " 'elevation lowest point': 'Amu Darya 258 m',\n",
       " 'economy': {'GDP': '$20.24 billion (2017 est.)',\n",
       "  'GDP-composition': {'agriculture': '23% (2016 est.)',\n",
       "   'industry': '21.1% (2016 est.)',\n",
       "   'services': '55.9% (2016 est.)'}},\n",
       " 'education': {'school life expectancy': '10 years',\n",
       "  'Education expenditures': '2.9% of GDP (2020 est.)'},\n",
       " 'health': {'life expectancy at birth': {'total population:': '54.05 years',\n",
       "   'male:': '52.47 years',\n",
       "   'female:': '55.71 years (2023 est.)'},\n",
       "  'infant mortality rate': {'total:': '103.06 deaths/1,000 live births',\n",
       "   'male:': '111.47 deaths/1,000 live births',\n",
       "   'female:': '94.24 deaths/1,000 live births (2023 est.)'},\n",
       "  'Health problems': {'Current health expenditure': '15.5% of GDP (2020)',\n",
       "   'Physicians density': '0.25 physicians/1,000 population (2020)',\n",
       "   'Hospital bed density': '0.4 beds/1,000 population (2017)'}}}"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data['afghanistan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c110ac10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan\\n',\n",
       " 'Akrotiri\\n',\n",
       " 'Albania\\n',\n",
       " 'Algeria\\n',\n",
       " 'American Samoa\\n',\n",
       " 'Andorra\\n',\n",
       " 'Angola\\n',\n",
       " 'Anguilla\\n',\n",
       " 'Antarctica\\n',\n",
       " 'Antigua and Barbuda\\n',\n",
       " 'Argentina\\n',\n",
       " 'Armenia\\n',\n",
       " 'Aruba\\n',\n",
       " 'Ashmore and Cartier Islands\\n',\n",
       " 'Australia\\n',\n",
       " 'Austria\\n',\n",
       " 'Azerbaijan\\n',\n",
       " 'Bahamas, The\\n',\n",
       " 'Bahrain\\n',\n",
       " 'Baker Island\\n',\n",
       " 'Bangladesh\\n',\n",
       " 'Barbados\\n',\n",
       " 'Belarus\\n',\n",
       " 'Belgium\\n',\n",
       " 'Belize\\n',\n",
       " 'Benin\\n',\n",
       " 'Bermuda\\n',\n",
       " 'Bhutan\\n',\n",
       " 'Bosnia and Herzegovina\\n',\n",
       " 'Botswana\\n',\n",
       " 'Bouvet Island\\n',\n",
       " 'Brazil\\n',\n",
       " 'British Indian Ocean Territory\\n',\n",
       " 'British Virgin Islands\\n',\n",
       " 'Brunei\\n',\n",
       " 'Bulgaria\\n',\n",
       " 'Burkina Faso\\n',\n",
       " 'Burma\\n',\n",
       " 'Burundi\\n',\n",
       " 'Cabo Verde\\n',\n",
       " 'Cambodia\\n',\n",
       " 'Cameroon\\n',\n",
       " 'Canada\\n',\n",
       " 'Cayman Islands\\n',\n",
       " 'Central African Republic\\n',\n",
       " 'Chad\\n',\n",
       " 'Chile\\n',\n",
       " 'China\\n',\n",
       " 'Christmas Island\\n',\n",
       " 'Clipperton Island\\n',\n",
       " 'Cocos (Keeling) Islands\\n',\n",
       " 'Colombia\\n',\n",
       " 'Comoros\\n',\n",
       " 'Congo, Democratic Republic of the\\n',\n",
       " 'Congo, Republic of the\\n',\n",
       " 'Cook Islands\\n',\n",
       " 'Coral Sea Islands\\n',\n",
       " 'Costa Rica\\n',\n",
       " 'Cote d’Ivoire\\n',\n",
       " 'Croatia\\n',\n",
       " 'Cuba\\n',\n",
       " 'Curacao\\n',\n",
       " 'Cyprus\\n',\n",
       " 'Czechia\\n',\n",
       " 'Denmark\\n',\n",
       " 'Dhekelia\\n',\n",
       " 'Djibouti\\n',\n",
       " 'Dominica\\n',\n",
       " 'Dominican Republic\\n',\n",
       " 'Ecuador\\n',\n",
       " 'Egypt\\n',\n",
       " 'El Salvador\\n',\n",
       " 'Equatorial Guinea\\n',\n",
       " 'Eritrea\\n',\n",
       " 'Estonia\\n',\n",
       " 'Eswatini\\n',\n",
       " 'Ethiopia\\n',\n",
       " 'European Union\\n',\n",
       " 'Falkland Islands (Islas Malvinas)\\n',\n",
       " 'Faroe Islands\\n',\n",
       " 'Fiji\\n',\n",
       " 'Finland\\n',\n",
       " 'France\\n',\n",
       " 'French Polynesia\\n',\n",
       " 'French Southern and Antarctic Lands\\n',\n",
       " 'Gabon\\n',\n",
       " 'Gambia, The\\n',\n",
       " 'Gaza Strip\\n',\n",
       " 'Georgia\\n',\n",
       " 'Germany\\n',\n",
       " 'Ghana\\n',\n",
       " 'Gibraltar\\n',\n",
       " 'Greece\\n',\n",
       " 'Greenland\\n',\n",
       " 'Grenada\\n',\n",
       " 'Guam\\n',\n",
       " 'Guatemala\\n',\n",
       " 'Guernsey\\n',\n",
       " 'Guinea\\n',\n",
       " 'Guinea-Bissau\\n',\n",
       " 'Guyana\\n',\n",
       " 'Haiti\\n',\n",
       " 'Heard Island and McDonald Islands\\n',\n",
       " 'Holy See (Vatican City)\\n',\n",
       " 'Honduras\\n',\n",
       " 'Hong Kong\\n',\n",
       " 'Howland Island\\n',\n",
       " 'Hungary\\n',\n",
       " 'Iceland\\n',\n",
       " 'India\\n',\n",
       " 'Indonesia\\n',\n",
       " 'Iran\\n',\n",
       " 'Iraq\\n',\n",
       " 'Ireland\\n',\n",
       " 'Isle of Man\\n',\n",
       " 'Israel\\n',\n",
       " 'Italy\\n',\n",
       " 'Jamaica\\n',\n",
       " 'Jan Mayen\\n',\n",
       " 'Japan\\n',\n",
       " 'Jarvis Island\\n',\n",
       " 'Jersey\\n',\n",
       " 'Johnston Atoll\\n',\n",
       " 'Jordan\\n',\n",
       " 'Kazakhstan\\n',\n",
       " 'Kenya\\n',\n",
       " 'Kingman Reef\\n',\n",
       " 'Kiribati\\n',\n",
       " 'Korea, North\\n',\n",
       " 'Korea, South\\n',\n",
       " 'Kosovo\\n',\n",
       " 'Kuwait\\n',\n",
       " 'Kyrgyzstan\\n',\n",
       " 'Laos\\n',\n",
       " 'Latvia\\n',\n",
       " 'Lebanon\\n',\n",
       " 'Lesotho\\n',\n",
       " 'Liberia\\n',\n",
       " 'Libya\\n',\n",
       " 'Liechtenstein\\n',\n",
       " 'Lithuania\\n',\n",
       " 'Luxembourg\\n',\n",
       " 'Macau\\n',\n",
       " 'Madagascar\\n',\n",
       " 'Malawi\\n',\n",
       " 'Malaysia\\n',\n",
       " 'Maldives\\n',\n",
       " 'Mali\\n',\n",
       " 'Malta\\n',\n",
       " 'Marshall Islands\\n',\n",
       " 'Mauritania\\n',\n",
       " 'Mauritius\\n',\n",
       " 'Mexico\\n',\n",
       " 'Micronesia, Federated States of\\n',\n",
       " 'Midway Islands\\n',\n",
       " 'Moldova\\n',\n",
       " 'Monaco\\n',\n",
       " 'Mongolia\\n',\n",
       " 'Montenegro\\n',\n",
       " 'Montserrat\\n',\n",
       " 'Morocco\\n',\n",
       " 'Mozambique\\n',\n",
       " 'Namibia\\n',\n",
       " 'Nauru\\n',\n",
       " 'Navassa Island\\n',\n",
       " 'Nepal\\n',\n",
       " 'Netherlands\\n',\n",
       " 'New Caledonia\\n',\n",
       " 'New Zealand\\n',\n",
       " 'Nicaragua\\n',\n",
       " 'Niger\\n',\n",
       " 'Nigeria\\n',\n",
       " 'Niue\\n',\n",
       " 'Norfolk Island\\n',\n",
       " 'North Macedonia\\n',\n",
       " 'Northern Mariana Islands\\n',\n",
       " 'Norway\\n',\n",
       " 'Oman\\n',\n",
       " 'Pakistan\\n',\n",
       " 'Palau\\n',\n",
       " 'Palmyra Atoll\\n',\n",
       " 'Panama\\n',\n",
       " 'Papua New Guinea\\n',\n",
       " 'Paracel Islands\\n',\n",
       " 'Paraguay\\n',\n",
       " 'Peru\\n',\n",
       " 'Philippines\\n',\n",
       " 'Pitcairn Islands\\n',\n",
       " 'Poland\\n',\n",
       " 'Portugal\\n',\n",
       " 'Puerto Rico\\n',\n",
       " 'Qatar\\n',\n",
       " 'Romania\\n',\n",
       " 'Russia\\n',\n",
       " 'Rwanda\\n',\n",
       " 'Saint Barthelemy\\n',\n",
       " 'Saint Helena, Ascension, and Tristan da Cunha\\n',\n",
       " 'Saint Kitts and Nevis\\n',\n",
       " 'Saint Lucia\\n',\n",
       " 'Saint Martin\\n',\n",
       " 'Saint Pierre and Miquelon\\n',\n",
       " 'Saint Vincent and the Grenadines\\n',\n",
       " 'Samoa\\n',\n",
       " 'San Marino\\n',\n",
       " 'Sao Tome and Principe\\n',\n",
       " 'Saudi Arabia\\n',\n",
       " 'Senegal\\n',\n",
       " 'Taiwan\\n',\n",
       " 'Tajikistan\\n',\n",
       " 'Tanzania\\n',\n",
       " 'Thailand\\n',\n",
       " 'Timor-Leste\\n',\n",
       " 'Togo\\n',\n",
       " 'Tokelau\\n',\n",
       " 'Tonga\\n',\n",
       " 'Trinidad and Tobago\\n',\n",
       " 'Tunisia\\n',\n",
       " 'Turkey (Turkiye)\\n',\n",
       " 'Turkmenistan\\n',\n",
       " 'Turks and Caicos Islands\\n',\n",
       " 'Tuvalu\\n',\n",
       " 'Uganda\\n',\n",
       " 'Ukraine\\n',\n",
       " 'United Arab Emirates\\n',\n",
       " 'United Kingdom\\n',\n",
       " 'United States\\n',\n",
       " 'United States Pacific Island Wildlife Refuges\\n',\n",
       " 'Uruguay\\n',\n",
       " 'Uzbekistan\\n',\n",
       " 'Vanuatu\\n',\n",
       " 'Venezuela\\n',\n",
       " 'Vietnam\\n',\n",
       " 'Virgin Islands\\n',\n",
       " 'Wake Island\\n',\n",
       " 'Wallis and Futuna\\n',\n",
       " 'West Bank\\n',\n",
       " 'World\\n',\n",
       " 'Yemen\\n',\n",
       " 'Zambia\\n',\n",
       " 'Zimbabwe']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAWCountries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
